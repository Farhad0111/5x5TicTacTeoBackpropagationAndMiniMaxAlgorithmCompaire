# 5x5 Tic-Tac-Toe Game with AI ğŸ®

A complete 5x5 Tic-Tac-Toe game with beautiful GUI featuring two different AI implementations: MiniMax Algorithm and Neural Network with Backpropagation.

## ğŸš€ Quick Start

### Running the Game

Simply run the main launcher:

```bash
python main_launcher.py
```

Or run the GUI directly:

```bash
python gui_launcher.py
```

## ğŸ¯ Game Features

### Main Menu
- **Intuitive GUI** with easy game mode selection
- **Beautiful Design** with color-coded elements
- **Two AI Options** to choose from

### Game Modes

#### 1. ğŸ¤– MiniMax Algorithm
- Classic AI using MiniMax algorithm with Alpha-Beta pruning
- Perfect strategic play
- Challenging opponent that thinks several moves ahead
- Fast decision making

#### 2. ğŸ§  Neural Network (Backpropagation)
- AI trained with Neural Network using Backpropagation
- Learning-based approach
- Adaptive gameplay
- Improves with training

## ğŸ² How to Play

1. **Launch the Game**: Run `python main_launcher.py`
2. **Select Game Mode**: Choose between MiniMax AI or Neural Network AI
3. **Play**: 
   - You are **X** (Red)
   - AI is **O** (Blue)
   - Click on any empty cell to make your move
   - Get **3 in a row** (horizontal, vertical, or diagonal) to win!
4. **New Game**: Click "New Game" button to restart
5. **Back to Menu**: Return to main menu to select a different AI

## ğŸ“‹ Requirements

- Python 3.7 or higher
- tkinter (usually included with Python)
- numpy (for Neural Network mode)

### Installing Dependencies

```bash
pip install numpy
```

For the Neural Network mode, you can also install additional dependencies:

```bash
cd "Backpropagation Algorithm"
pip install -r requirements.txt
```

## ğŸ¨ GUI Features

### Main Menu
- Clean and modern design
- Easy-to-understand game mode descriptions
- Color-coded options (Green for MiniMax, Blue for Neural Network)

### Game Board
- 5x5 grid with clear cell boundaries
- Visual feedback for moves
- Color-coded pieces (Red for player, Blue for AI)
- Status messages showing whose turn it is
- Win/Loss/Tie notifications

### Controls
- **New Game**: Start a fresh game with the same AI
- **Back to Menu**: Return to mode selection screen
- **Exit**: Close the application

## ğŸ§  How the AI Algorithms Work

### ğŸ¤– MiniMax Algorithm (Game Tree Search)

**Location:** `MiniMax Algorithm/FinalTicTacTeo.py`

#### **What is MiniMax?**
MiniMax is a classic game theory algorithm that assumes both players play optimally. It explores the game tree to find the best move by simulating all possible future moves.

#### **How It Works:**

1. **Game Tree Exploration**
   ```
   Current Position
   â”œâ”€ Move 1 (AI)
   â”‚  â”œâ”€ Move A (Player)
   â”‚  â”‚  â”œâ”€ Move X (AI) â†’ Score: +5
   â”‚  â”‚  â””â”€ Move Y (AI) â†’ Score: +3
   â”‚  â””â”€ Move B (Player)
   â”‚     â”œâ”€ Move X (AI) â†’ Score: -2
   â”‚     â””â”€ Move Y (AI) â†’ Score: +1
   â””â”€ Move 2 (AI)
      â””â”€ ... (continues)
   ```

2. **Recursive Evaluation**
   - **Maximizing Player (AI/O):** Tries to maximize the score
   - **Minimizing Player (Human/X):** Tries to minimize the score
   - Recursively evaluates all possible game states up to a certain depth

3. **Alpha-Beta Pruning Optimization**
   - Cuts off branches that won't affect the final decision
   - Significantly reduces the number of positions evaluated
   - Makes the algorithm much faster without losing accuracy
   
   Example:
   ```
   If we already found a move with score +5,
   and a branch shows it can only get worse (< +5),
   we stop exploring that branch (prune it).
   ```

4. **Position Scoring**
   - **Win (AI):** +100 + depth (favors faster wins)
   - **Loss (AI):** -100 - depth (avoids faster losses)
   - **Draw:** 0
   - **Ongoing:** Evaluates based on potential winning patterns

5. **Move Selection**
   - AI always picks the move with the highest evaluated score
   - Guarantees optimal play within search depth
   - Very difficult to beat!

#### **Strengths:**
- âœ… Deterministic and consistent
- âœ… Plays optimally within search depth
- âœ… No training required
- âœ… Guaranteed to find winning moves

#### **Limitations:**
- âš ï¸ Computationally expensive for deep searches
- âš ï¸ Limited by search depth (default: 4 moves ahead)
- âš ï¸ Performance degrades on larger boards

---

### ğŸ§  Neural Network with Backpropagation (Machine Learning)

**Location:** `Backpropagation Algorithm/`

#### **What is a Neural Network?**
A Neural Network is a machine learning model inspired by the human brain. It learns patterns from data through training and can make decisions based on what it has learned.

#### **Network Architecture:**

```
Input Layer (25 neurons)
    â”‚  Each neuron = one cell on 5x5 board
    â”‚  Values: 1 (AI/O), -1 (Player/X), 0 (Empty)
    â†“
Hidden Layer (50 neurons)
    â”‚  With ReLU activation: f(x) = max(0, x)
    â”‚  Learns complex patterns and strategies
    â†“
Output Layer (1 neuron)
    â”‚  With Tanh activation: f(x) = (e^x - e^-x)/(e^x + e^-x)
    â”‚  Output: Board evaluation score (-1 to +1)
    â””â†’ Higher = Better for AI, Lower = Better for Player
```

#### **How It Works:**

1. **Board Representation**
   ```python
   5x5 Board:        Flattened Input Vector:
   [X, O, -, -, -]   [âˆ’1, 1, 0, 0, 0,
   [-, X, O, -, -]    0, âˆ’1, 1, 0, 0,
   [-, -, X, -, -] â†’  0, 0, âˆ’1, 0, 0,
   [O, -, -, -, -]    1, 0, 0, 0, 0,
   [-, -, -, -, O]    0, 0, 0, 0, 1]
   ```

2. **Forward Pass (Making a Prediction)**
   ```
   Step 1: Input â†’ Hidden Layer
   hidden = ReLU(input Ã— weights1 + bias1)
   
   Step 2: Hidden â†’ Output Layer
   output = Tanh(hidden Ã— weights2 + bias2)
   
   Result: Score indicating how good the position is
   ```

3. **Move Selection**
   - Evaluates all possible moves
   - Each move creates a new board state
   - Forward pass computes score for each state
   - Selects move with highest score
   - Also checks for immediate wins and blocks opponent wins

4. **Backpropagation (Learning Process)**
   
   When training, the network learns from game outcomes:
   
   ```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. Play a game and get result      â”‚
   â”‚     Win: +1, Loss: -1, Draw: 0      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  2. Calculate Error                 â”‚
   â”‚     Error = Actual - Predicted      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  3. Backward Pass                   â”‚
   â”‚     Compute gradient for each weightâ”‚
   â”‚     using chain rule                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  4. Update Weights                  â”‚
   â”‚     weight = weight - (learning_rateâ”‚
   â”‚              Ã— gradient)            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  5. Repeat for many games           â”‚
   â”‚     Network gradually improves!     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   ```

5. **Training Process**
   - Plays random games to generate training data
   - Each game provides examples of board positions and outcomes
   - Backpropagation adjusts weights to improve predictions
   - More training = Better performance

#### **Key Formulas:**

**ReLU Activation (Hidden Layer):**
```
f(x) = max(0, x)
f'(x) = 1 if x > 0, else 0
```

**Tanh Activation (Output Layer):**
```
f(x) = (e^x - e^-x) / (e^x + e^-x)
f'(x) = 1 - f(x)Â²
```

**Weight Update Rule:**
```
weight_new = weight_old - learning_rate Ã— âˆ‚Error/âˆ‚weight
```

#### **Strengths:**
- âœ… Can learn complex patterns from experience
- âœ… Adapts and improves with training
- âœ… Generalizes to new positions
- âœ… Fast evaluation (no tree search)

#### **Limitations:**
- âš ï¸ Requires training to play well
- âš ï¸ Quality depends on training data
- âš ï¸ Less predictable than MiniMax
- âš ï¸ May not play optimally without extensive training

---

### ğŸ“Š Comparison

| Feature | MiniMax | Neural Network |
|---------|---------|----------------|
| **Approach** | Rule-based search | Learning-based |
| **Training** | None required | Requires training |
| **Consistency** | Always optimal | Varies with training |
| **Speed** | Slower (tree search) | Faster (direct evaluation) |
| **Scalability** | Limited by depth | Scales better |
| **Explainability** | High (can trace moves) | Low (black box) |
| **Difficulty** | Very challenging | Moderate (if trained) |

### ğŸ¯ Which Algorithm to Choose?

- **Choose MiniMax** if you want:
  - Maximum challenge
  - Consistent difficulty
  - Guaranteed optimal play

- **Choose Neural Network** if you want:
  - To see machine learning in action
  - Adaptive gameplay
  - Faster move calculations

## ğŸ“ Project Structure

```
5x5TicTacTeo/
â”œâ”€â”€ main_launcher.py              # Main entry point
â”œâ”€â”€ gui_launcher.py               # GUI implementation
â”œâ”€â”€ README_GUI.md                 # This file
â”œâ”€â”€ MiniMax Algorithm/
â”‚   â””â”€â”€ FinalTicTacTeo.py        # MiniMax AI implementation
â””â”€â”€ Backpropagation Algorithm/
    â”œâ”€â”€ game_logic.py            # Game logic and NN AI
    â”œâ”€â”€ neural_network.py        # Neural Network implementation
    â”œâ”€â”€ quick_start.py           # Quick start guide
    â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ“ Training the Neural Network
If you want to improve the Neural Network AI:

```bash
cd "Backpropagation Algorithm"
python game_logic.py train 2000 300
```

This will train the AI with 2000 games and 300 epochs.

## ğŸ† Game Rules
- **Board**: 5x5 grid
- **Goal**: Get 3 of your symbols in a row
- **Directions**: Horizontal, Vertical, or Diagonal
- **Players**: You (X) vs AI (O)
- **Turn Order**: Player always goes first

## ğŸ› Troubleshooting

### GUI doesn't start
- Make sure tkinter is installed (comes with most Python installations)
- On Linux: `sudo apt-get install python3-tk`

### Neural Network not working
- Ensure numpy is installed: `pip install numpy`
- If "No trained model found" message appears, the AI will use random initialization (train it for better performance)

### Import errors
- Make sure you're running the script from the correct directory
- Try: `cd "c:\Users\billa\OneDrive\Desktop\5x5TicTacTeo"`

## ğŸ“ Notes
- The Neural Network AI may not play optimally if untrained
- Training the Neural Network improves its performance
- MiniMax AI provides consistent challenging gameplay
- Both AIs can be beaten with good strategy!

## ğŸ‰ Enjoy the Game!
Have fun playing 5x5 Tic-Tac-Toe with AI opponents!
#   5 x 5 T i c T a c T e o B a c k p r o p a g a t i o n A n d M i n i M a x A l g o r i t h m C o m p a i r e  
 